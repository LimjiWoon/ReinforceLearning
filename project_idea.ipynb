{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOZgwM16DlvgXWDq9RuDXxX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# Modify the import statement in maze library if possible\n","from collections.abc import ValuesView"],"metadata":{"id":"0NrRLLxEbXWP","executionInfo":{"status":"ok","timestamp":1716517701816,"user_tz":-540,"elapsed":445,"user":{"displayName":"임지운","userId":"15598219104552818011"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["import gym\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from gym import spaces\n","import random\n","\n","class EndoscopyEnv(gym.Env):\n","    def __init__(self):\n","        super(EndoscopyEnv, self).__init__()\n","\n","        # Define action and observation space\n","        self.action_space = spaces.Discrete(182)  # 0-179 degrees rotation, 180: forward, 181: backward\n","\n","        # Observation space: x, y position, orientation angle, front sensor readings\n","        self.observation_space = spaces.Box(low=np.array([0, 0, 0, 0]), high=np.array([100, 100, 180, 1]), dtype=np.float32)\n","\n","        # Initial state\n","        self.state = np.array([1, 1, 90, 0], dtype=np.float32)  # x, y, orientation, front sensor reading\n","        self.goal = np.array([8, 8])  # goal at the bottom right\n","\n","        self.max_steps = 200\n","        self.current_step = 0\n","        self.grid_size = 10  # Grid size to create the maze\n","        self.maze = self._create_maze()\n","\n","    def _create_maze(self):\n","        maze = np.ones((self.grid_size, self.grid_size), dtype=int)\n","        start = (1, 1)\n","        end = (self.grid_size - 2, self.grid_size - 2)\n","\n","        # Ensure start and end points are clear\n","        maze[start[0]][start[1]] = 0\n","        maze[end[0]][end[1]] = 0\n","\n","        stack = [start]\n","        directions = [(0, 1), (1, 0), (0, -1), (-1, 0)]\n","\n","        while stack:\n","            current = stack[-1]\n","            neighbors = []\n","\n","            for direction in directions:\n","                nx, ny = current[0] + direction[0], current[1] + direction[1]\n","                if 1 <= nx < self.grid_size - 1 and 1 <= ny < self.grid_size - 1 and maze[nx][ny] == 1:\n","                    neighbors.append((nx, ny))\n","\n","            if neighbors:\n","                next_cell = random.choice(neighbors)\n","                maze[next_cell[0]][next_cell[1]] = 0\n","                maze[(current[0] + next_cell[0]) // 2][(current[1] + next_cell[1]) // 2] = 0\n","                stack.append(next_cell)\n","            else:\n","                stack.pop()\n","\n","        return maze\n","\n","    def reset(self):\n","        self.state = np.array([1, 1, 90, 0], dtype=np.float32)\n","        self.current_step = 0\n","        self.maze = self._create_maze()\n","        return self.state\n","\n","    def step(self, action):\n","        self.current_step += 1\n","\n","        if action == 180:  # move forward\n","            self.state[1] += np.cos(np.radians(self.state[2]))\n","            self.state[0] += np.sin(np.radians(self.state[2]))\n","        elif action == 181:  # move backward\n","            self.state[1] -= np.cos(np.radians(self.state[2]))\n","            self.state[0] -= np.sin(np.radians(self.state[2]))\n","        else:  # rotate\n","            self.state[2] = (self.state[2] + action) % 180\n","\n","        # Update sensor reading (simplified for this example)\n","        self.state[3] = self._get_sensor_reading()\n","\n","        done = np.linalg.norm(self.state[:2] - self.goal) < 1 or self.current_step >= self.max_steps\n","        reward = -1.0  # Penalize each step to encourage efficiency\n","\n","        if done:\n","            if np.linalg.norm(self.state[:2] - self.goal) < 1:\n","                reward = 100.0  # Reward reaching the goal\n","            else:\n","                reward = -100.0  # Penalize not reaching the goal within the maximum steps\n","\n","        info = {}\n","        return self.state, reward, done, info\n","\n","    def _get_sensor_reading(self):\n","        # Simplified sensor reading: distance to goal normalized\n","        distance = np.linalg.norm(self.state[:2] - self.goal)\n","        return 1.0 - (distance / np.linalg.norm(np.array([0, 0]) - self.goal))\n","\n","    def render(self, mode='human'):\n","        plt.figure(figsize=(10, 10))\n","        plt.imshow(self.maze, cmap='Greys', origin='upper', extent=(0, 100, 0, 100))\n","        plt.plot(self.state[1] * 10, self.state[0] * 10, 'bo', markersize=10)  # Endoscope position\n","        plt.plot(self.goal[1] * 10, self.goal[0] * 10, 'ro', markersize=10)  # Goal position\n","        plt.xlabel('X-axis')\n","        plt.ylabel('Y-axis')\n","        plt.title('Endoscopy Robot Environment')\n","        plt.grid()\n","        plt.show()\n","\n","# Instantiate and test the environment\n","env = EndoscopyEnv()\n","obs = env.reset()\n","\n","done = False\n","while not done:\n","    action = env.action_space.sample()  # Sample random action\n","    obs, reward, done, info = env.step(action)\n","    env.render()\n","    print(f\"State: {obs}, Reward: {reward}, Done: {done}\")\n"],"metadata":{"id":"LJT4cJC-Ym6P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"RHi0LeEnbe5m"},"execution_count":null,"outputs":[]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyMBIpwi9XYx5JJjZ0xkQqbG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"970aa09afdc7412bbb9a073ab0d4d2a1":{"model_module":"@jupyter-widgets/controls","model_name":"ImageModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ImageModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ImageView","format":"png","height":"","layout":"IPY_MODEL_a29a052ff3fa4ed59e0e8b2d87c07e3a","width":""}},"a29a052ff3fa4ed59e0e8b2d87c07e3a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eeea1b7e3f61486c8a6279401b7be463":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Next GIF","disabled":false,"icon":"","layout":"IPY_MODEL_e36f459f47e24d859b36f2476b6132f6","style":"IPY_MODEL_78aac96fc65a4ba497e08b58faa5bcbf","tooltip":""}},"e36f459f47e24d859b36f2476b6132f6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"78aac96fc65a4ba497e08b58faa5bcbf":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}}}}},"cells":[{"cell_type":"markdown","source":["# 바닐라 Actor-Critic"],"metadata":{"id":"EIkymux4fT6F"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"UTyGBtBsUKPY","executionInfo":{"status":"ok","timestamp":1715308024874,"user_tz":-540,"elapsed":5436,"user":{"displayName":"임지운","userId":"15598219104552818011"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import gym"]},{"cell_type":"code","source":["class Actor(nn.Module):\n","    def __init__(self, state_dim, action_dim, hidden_dim):\n","        super(Actor, self).__init__()\n","        self.fc1 = nn.Linear(state_dim, hidden_dim)\n","        self.fc2 = nn.Linear(hidden_dim, action_dim)\n","\n","    def forward(self, state):\n","        x = torch.relu(self.fc1(state))\n","        x = torch.tanh(self.fc2(x))\n","        return x\n","\n","class Critic(nn.Module):\n","    def __init__(self, state_dim, hidden_dim):\n","        super(Critic, self).__init__()\n","        self.fc1 = nn.Linear(state_dim, hidden_dim)\n","        self.fc2 = nn.Linear(hidden_dim, 1)\n","\n","    def forward(self, state):\n","        x = torch.relu(self.fc1(state))\n","        x = self.fc2(x)\n","        return x"],"metadata":{"id":"gQQUBwhdUxhz","executionInfo":{"status":"ok","timestamp":1715308026157,"user_tz":-540,"elapsed":2,"user":{"displayName":"임지운","userId":"15598219104552818011"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["def train_actor_critic(env, actor, critic, actor_optimizer, critic_optimizer, gamma, num_episodes, print_interval=10):\n","    episode_rewards = []\n","    for episode in range(num_episodes):\n","        state = env.reset()\n","        done = False\n","        total_reward = 0\n","        while not done:\n","            state = torch.FloatTensor(state)\n","            action_prob = actor(state)\n","            action_prob = torch.softmax(action_prob, dim=-1)\n","            action_dist = torch.distributions.Categorical(action_prob)\n","            action = action_dist.sample()\n","            next_state, reward, done, _ = env.step(action.item())\n","            next_state = torch.FloatTensor(next_state)\n","\n","            critic_value = critic(state)\n","            next_critic_value = critic(next_state)\n","            td_target = reward + gamma * next_critic_value * (1 - done)\n","            td_error = td_target - critic_value\n","\n","            actor_loss = -action_dist.log_prob(action) * td_error.detach()\n","            critic_loss = td_error.pow(2)\n","\n","            actor_optimizer.zero_grad()\n","            critic_optimizer.zero_grad()\n","            actor_loss.backward()\n","            critic_loss.backward()\n","            actor_optimizer.step()\n","            critic_optimizer.step()\n","\n","            state = next_state\n","            total_reward += reward\n","\n","        episode_rewards.append(total_reward)\n","\n","        if (episode + 1) % print_interval == 0:\n","            mean_reward = sum(episode_rewards[-print_interval:]) / print_interval\n","            print(f\"Episode {episode + 1}/{num_episodes}, Mean Reward: {mean_reward}\")\n"],"metadata":{"id":"bhyJmvTTU0ZZ","executionInfo":{"status":"ok","timestamp":1715308204370,"user_tz":-540,"elapsed":402,"user":{"displayName":"임지운","userId":"15598219104552818011"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["env = gym.make('CartPole-v1')\n","state_dim = env.observation_space.shape[0]\n","action_dim = env.action_space.n\n","hidden_dim = 128\n","\n","actor = Actor(state_dim, action_dim, hidden_dim)\n","critic = Critic(state_dim, hidden_dim)\n","actor_optimizer = optim.Adam(actor.parameters(), lr=0.001)\n","critic_optimizer = optim.Adam(critic.parameters(), lr=0.001)\n","gamma = 0.99\n","num_episodes = 500\n","\n","train_actor_critic(env, actor, critic, actor_optimizer, critic_optimizer, gamma, num_episodes)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3x_1mz0qU3G5","executionInfo":{"status":"ok","timestamp":1715309007505,"user_tz":-540,"elapsed":802101,"user":{"displayName":"임지운","userId":"15598219104552818011"}},"outputId":"abe80e5b-4203-44e2-ea6d-46172ae05ce7"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Episode 10/1000, Mean Reward: 32.0\n","Episode 20/1000, Mean Reward: 31.7\n","Episode 30/1000, Mean Reward: 29.6\n","Episode 40/1000, Mean Reward: 38.5\n","Episode 50/1000, Mean Reward: 50.3\n","Episode 60/1000, Mean Reward: 67.5\n","Episode 70/1000, Mean Reward: 106.3\n","Episode 80/1000, Mean Reward: 74.9\n","Episode 90/1000, Mean Reward: 37.2\n","Episode 100/1000, Mean Reward: 73.0\n","Episode 110/1000, Mean Reward: 93.7\n","Episode 120/1000, Mean Reward: 95.7\n","Episode 130/1000, Mean Reward: 99.1\n","Episode 140/1000, Mean Reward: 77.5\n","Episode 150/1000, Mean Reward: 116.5\n","Episode 160/1000, Mean Reward: 85.4\n","Episode 170/1000, Mean Reward: 75.3\n","Episode 180/1000, Mean Reward: 71.5\n","Episode 190/1000, Mean Reward: 156.2\n","Episode 200/1000, Mean Reward: 172.1\n","Episode 210/1000, Mean Reward: 66.6\n","Episode 220/1000, Mean Reward: 83.9\n","Episode 230/1000, Mean Reward: 95.2\n","Episode 240/1000, Mean Reward: 189.8\n","Episode 250/1000, Mean Reward: 165.7\n","Episode 260/1000, Mean Reward: 103.9\n","Episode 270/1000, Mean Reward: 202.4\n","Episode 280/1000, Mean Reward: 130.2\n","Episode 290/1000, Mean Reward: 109.5\n","Episode 300/1000, Mean Reward: 123.6\n","Episode 310/1000, Mean Reward: 116.1\n","Episode 320/1000, Mean Reward: 141.1\n","Episode 330/1000, Mean Reward: 129.5\n","Episode 340/1000, Mean Reward: 129.1\n","Episode 350/1000, Mean Reward: 120.8\n","Episode 360/1000, Mean Reward: 189.1\n","Episode 370/1000, Mean Reward: 183.8\n","Episode 380/1000, Mean Reward: 193.6\n","Episode 390/1000, Mean Reward: 160.7\n","Episode 400/1000, Mean Reward: 208.1\n","Episode 410/1000, Mean Reward: 133.4\n","Episode 420/1000, Mean Reward: 137.3\n","Episode 430/1000, Mean Reward: 146.1\n","Episode 440/1000, Mean Reward: 297.1\n","Episode 450/1000, Mean Reward: 429.0\n","Episode 460/1000, Mean Reward: 296.3\n","Episode 470/1000, Mean Reward: 369.8\n","Episode 480/1000, Mean Reward: 486.2\n","Episode 490/1000, Mean Reward: 404.9\n","Episode 500/1000, Mean Reward: 363.4\n","Episode 510/1000, Mean Reward: 328.0\n","Episode 520/1000, Mean Reward: 304.9\n","Episode 530/1000, Mean Reward: 375.2\n","Episode 540/1000, Mean Reward: 226.3\n","Episode 550/1000, Mean Reward: 339.3\n","Episode 560/1000, Mean Reward: 355.5\n","Episode 570/1000, Mean Reward: 197.3\n","Episode 580/1000, Mean Reward: 274.5\n","Episode 590/1000, Mean Reward: 402.5\n","Episode 600/1000, Mean Reward: 275.5\n","Episode 610/1000, Mean Reward: 142.3\n","Episode 620/1000, Mean Reward: 302.8\n","Episode 630/1000, Mean Reward: 345.1\n","Episode 640/1000, Mean Reward: 474.6\n","Episode 650/1000, Mean Reward: 464.5\n","Episode 660/1000, Mean Reward: 496.0\n","Episode 670/1000, Mean Reward: 321.8\n","Episode 680/1000, Mean Reward: 412.9\n","Episode 690/1000, Mean Reward: 500.0\n","Episode 700/1000, Mean Reward: 463.9\n","Episode 710/1000, Mean Reward: 405.7\n","Episode 720/1000, Mean Reward: 395.2\n","Episode 730/1000, Mean Reward: 413.5\n","Episode 740/1000, Mean Reward: 471.2\n","Episode 750/1000, Mean Reward: 489.6\n","Episode 760/1000, Mean Reward: 464.2\n","Episode 770/1000, Mean Reward: 419.5\n","Episode 780/1000, Mean Reward: 500.0\n","Episode 790/1000, Mean Reward: 283.6\n","Episode 800/1000, Mean Reward: 323.2\n","Episode 810/1000, Mean Reward: 429.4\n","Episode 820/1000, Mean Reward: 491.4\n","Episode 830/1000, Mean Reward: 468.2\n","Episode 840/1000, Mean Reward: 383.9\n","Episode 850/1000, Mean Reward: 341.2\n","Episode 860/1000, Mean Reward: 438.1\n","Episode 870/1000, Mean Reward: 405.3\n","Episode 880/1000, Mean Reward: 382.0\n","Episode 890/1000, Mean Reward: 401.0\n","Episode 900/1000, Mean Reward: 350.5\n","Episode 910/1000, Mean Reward: 263.0\n","Episode 920/1000, Mean Reward: 369.7\n","Episode 930/1000, Mean Reward: 418.1\n","Episode 940/1000, Mean Reward: 348.4\n","Episode 950/1000, Mean Reward: 329.5\n","Episode 960/1000, Mean Reward: 434.9\n","Episode 970/1000, Mean Reward: 362.4\n","Episode 980/1000, Mean Reward: 459.6\n","Episode 990/1000, Mean Reward: 406.5\n","Episode 1000/1000, Mean Reward: 479.2\n"]}]},{"cell_type":"code","source":["torch.save(actor.state_dict(), 'actor_model.pth')\n","torch.save(critic.state_dict(), 'critic_model.pth')"],"metadata":{"id":"kQW_f70mXkxS","executionInfo":{"status":"ok","timestamp":1715309012518,"user_tz":-540,"elapsed":269,"user":{"displayName":"임지운","userId":"15598219104552818011"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["actor = Actor(state_dim, action_dim, hidden_dim)\n","actor.load_state_dict(torch.load('actor_model.pth'))\n","actor.eval()  # 평가 모드로 설정 (드롭아웃 및 배치 정규화 등을 평가 모드로 설정)\n","\n","critic = Critic(state_dim, hidden_dim)\n","critic.load_state_dict(torch.load('critic_model.pth'))\n","critic.eval()  # 평가 모드로 설정 (드롭아웃 및 배치 정규화 등을 평가 모드로 설정)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_v7T81MmU5Qx","executionInfo":{"status":"ok","timestamp":1715309014370,"user_tz":-540,"elapsed":257,"user":{"displayName":"임지운","userId":"15598219104552818011"}},"outputId":"bbea093c-686b-4d14-f966-60c1c42471ba"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Critic(\n","  (fc1): Linear(in_features=4, out_features=128, bias=True)\n","  (fc2): Linear(in_features=128, out_features=1, bias=True)\n",")"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["import gym\n","import torch\n","import numpy as np\n","import imageio\n","\n","# 환경 설정\n","env = gym.make('CartPole-v1')\n","state_dim = env.observation_space.shape[0]\n","action_dim = env.action_space.n\n","hidden_dim = 128\n","\n","# 모델 불러오기\n","actor = Actor(state_dim, action_dim, hidden_dim)\n","actor.load_state_dict(torch.load('actor_model.pth'))\n","actor.eval()\n","\n","# 테스트를 위한 함수 정의\n","def test_actor(env, actor, num_episodes=10):\n","    for episode in range(num_episodes):\n","        state = env.reset()\n","        done = False\n","        frames = []  # 프레임을 저장할 리스트\n","        while not done:\n","            frames.append(env.render(mode='rgb_array'))  # 시각화용 프레임 저장\n","            state = torch.FloatTensor(state)\n","            action_prob = actor(state)\n","            action_prob = torch.softmax(action_prob, dim=-1)\n","            action = torch.argmax(action_prob).item()\n","            state, _, done, _ = env.step(action)\n","        env.close()\n","        save_video(frames, f'test_episode_{episode}.gif')  # 영상 저장\n","\n","# 영상 저장 함수 정의\n","def save_video(frames, filename, fps=30):\n","    imageio.mimsave(filename, [np.array(frame) for frame in frames], fps=fps)\n","\n","# 테스트 실행\n","test_actor(env, actor)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kzuH6rrDY267","executionInfo":{"status":"ok","timestamp":1715309324500,"user_tz":-540,"elapsed":185376,"user":{"displayName":"임지운","userId":"15598219104552818011"}},"outputId":"b1ac64e4-859a-4d67-ae63-3359a508d095"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/imageio/plugins/pillow.py:390: DeprecationWarning: The keyword `fps` is no longer supported. Use `duration`(in ms) instead, e.g. `fps=50` == `duration=20` (1000 * 1/50).\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["from IPython.display import display, Image\n","import os\n","import ipywidgets as widgets\n","\n","def display_gif_sequentially(folder_path):\n","    filenames = sorted(file for file in os.listdir(folder_path) if file.endswith('.gif'))\n","    image_widget = widgets.Image()\n","    display(image_widget)\n","\n","    index = 0\n","\n","    def update_image(change):\n","        nonlocal index\n","        index += 1\n","        if index >= len(filenames):\n","            index = 0\n","        image_path = os.path.join(folder_path, filenames[index])\n","        with open(image_path, 'rb') as f:\n","            image_widget.value = f.read()\n","\n","    next_button = widgets.Button(description='Next GIF')\n","    next_button.on_click(update_image)\n","    display(next_button)\n","\n","# 저장된 GIF 파일이 있는 폴더 경로\n","folder_path = '/content/'\n","\n","# GIF 파일을 순차적으로 보여줌\n","display_gif_sequentially(folder_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":456,"referenced_widgets":["970aa09afdc7412bbb9a073ab0d4d2a1","a29a052ff3fa4ed59e0e8b2d87c07e3a","eeea1b7e3f61486c8a6279401b7be463","e36f459f47e24d859b36f2476b6132f6","78aac96fc65a4ba497e08b58faa5bcbf"]},"id":"0jCzLwKfam47","executionInfo":{"status":"ok","timestamp":1715309747028,"user_tz":-540,"elapsed":556,"user":{"displayName":"임지운","userId":"15598219104552818011"}},"outputId":"11ca4af1-857e-4952-fac9-798fbb5eeac1"},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":["Image(value=b'')"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"970aa09afdc7412bbb9a073ab0d4d2a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Button(description='Next GIF', style=ButtonStyle())"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eeea1b7e3f61486c8a6279401b7be463"}},"metadata":{}}]},{"cell_type":"markdown","source":["# Advantage Actor-Critic"],"metadata":{"id":"98mV9YyGfYh9"}},{"cell_type":"code","source":["import gym\n","import torch\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.distributions import Categorical"],"metadata":{"id":"DkpF8sBJcTe7","executionInfo":{"status":"ok","timestamp":1715309985528,"user_tz":-540,"elapsed":261,"user":{"displayName":"임지운","userId":"15598219104552818011"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["# Actor 신경망 정의\n","class Actor(torch.nn.Module):\n","    def __init__(self, state_dim, action_dim, hidden_dim):\n","        super(Actor, self).__init__()\n","        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n","        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)\n","\n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        x = self.fc2(x)\n","        action_prob = F.softmax(x, dim=-1)  # softmax 함수를 이용하여 각 행동에 대한 확률 생성\n","        return action_prob\n","\n","\n","# Critic 신경망 정의\n","class Critic(torch.nn.Module):\n","    def __init__(self, state_dim, hidden_dim):\n","        super(Critic, self).__init__()\n","        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n","        self.fc2 = torch.nn.Linear(hidden_dim, 1)\n","\n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        x = self.fc2(x)\n","        return x"],"metadata":{"id":"zNYiW_1YcWL7","executionInfo":{"status":"ok","timestamp":1715310158211,"user_tz":-540,"elapsed":253,"user":{"displayName":"임지운","userId":"15598219104552818011"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["def compute_gae(next_value, rewards, masks, values, gamma, lam):\n","    values = values + [next_value]\n","    gae = 0\n","    returns = []\n","\n","    for step in reversed(range(len(rewards))):\n","        delta = rewards[step] + gamma * values[step + 1] * masks[step] - values[step]\n","        gae = delta + gamma * lam * masks[step] * gae\n","        returns.insert(0, gae + values[step])\n","\n","    return returns"],"metadata":{"id":"AfofIL1MZaBj","executionInfo":{"status":"ok","timestamp":1715310024111,"user_tz":-540,"elapsed":2,"user":{"displayName":"임지운","userId":"15598219104552818011"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["def train_actor_critic_with_gae(env, actor, critic, actor_optimizer, critic_optimizer, gamma, lam, num_episodes):\n","    for episode in range(num_episodes):\n","        log_probs = []\n","        values = []\n","        rewards = []\n","        masks = []\n","        entropy = 0\n","        episode_reward = 0  # 각 에피소드의 총 보상을 기록하기 위한 변수\n","\n","        state = env.reset()\n","        state = torch.FloatTensor(state)\n","\n","        for t in range(MAX_EPISODE_LENGTH):\n","            action_prob = actor(state)\n","            value = critic(state)\n","\n","            action_dist = Categorical(action_prob)\n","            action = action_dist.sample()\n","            next_state, reward, done, _ = env.step(action.item())\n","\n","            log_prob = action_dist.log_prob(action)\n","            entropy += action_dist.entropy().mean()\n","\n","            log_probs.append(log_prob.unsqueeze(0))  # 텐서를 리스트에 추가할 때 unsqueeze를 사용하여 차원을 추가\n","            values.append(value)\n","            rewards.append(reward)\n","            masks.append(1 - done)\n","\n","            episode_reward += reward  # 에피소드의 총 보상 업데이트\n","\n","            state = torch.FloatTensor(next_state)\n","\n","            if done:\n","                print(f\"Episode {episode}: Total Reward = {episode_reward}\")  # 각 에피소드의 총 보상 출력\n","                next_value = torch.tensor([0.0], device=device)\n","                break\n","\n","        next_value = critic(state)\n","        returns = compute_gae(next_value, rewards, masks, values, gamma, lam)\n","\n","        # 리스트를 텐서로 변환 후 연결\n","        log_probs = torch.cat(log_probs, dim=0)\n","        returns = torch.cat(returns, dim=0).detach()\n","        values = torch.cat(values, dim=0)\n","\n","        advantage = returns - values\n","\n","        actor_loss = -(log_probs * advantage.detach()).mean()\n","        critic_loss = F.mse_loss(returns, values)\n","\n","        actor_optimizer.zero_grad()\n","        critic_optimizer.zero_grad()\n","        actor_loss.backward()\n","        critic_loss.backward()\n","        actor_optimizer.step()\n","        critic_optimizer.step()\n"],"metadata":{"id":"Dn2RnBJ7YmmS","executionInfo":{"status":"ok","timestamp":1715310382419,"user_tz":-540,"elapsed":4,"user":{"displayName":"임지운","userId":"15598219104552818011"}}},"execution_count":42,"outputs":[]},{"cell_type":"code","source":["# 하이퍼파라미터 설정\n","MAX_EPISODE_LENGTH = 1000\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# 환경 설정\n","env = gym.make('CartPole-v1')\n","state_dim = env.observation_space.shape[0]\n","action_dim = env.action_space.n\n","hidden_dim = 128"],"metadata":{"id":"LOzVi_DvcgjU","executionInfo":{"status":"ok","timestamp":1715310382692,"user_tz":-540,"elapsed":2,"user":{"displayName":"임지운","userId":"15598219104552818011"}}},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":["actor_optimizer = optim.Adam(actor.parameters(), lr=0.01) </br>\n","critic_optimizer = optim.Adam(critic.parameters(), lr=0.01) </br>\n","해당 코드 부분에서 lr을 0.0001, 0.001, 0.01, 0.1로 학습해보았고 0.001은 느리지만 학습이 되었고 0.0001, 0.1은 학습 아예 안됬다 0.01은 빠르게 학습할 수 있어 best다.</br>\n","덕분에 에피소드를 1000에서 150까지 줄일 수 있었다. </br>\n","감마의 경우 미래의 보상이 더 중점을 두어 0.99로 </br>\n","람다의 경우 advantage의 가중치이므로 0.95로 </br>\n","각각 설정했지만 0~1사이값으로 테스트를 해보았다. </br>\n","하지만 0.9 아래의 값으로 내려가자 둘다 학습이 전혀 안되어 0.9 ~ 1 사이값이 가장 좋다는 것을 알았다."],"metadata":{"id":"GsNnRAyUhMXU"}},{"cell_type":"code","source":["# 모델 및 옵티마이저 초기화\n","actor = Actor(state_dim, action_dim, hidden_dim).to(device)\n","critic = Critic(state_dim, hidden_dim).to(device)\n","actor_optimizer = optim.Adam(actor.parameters(), lr=0.01)\n","critic_optimizer = optim.Adam(critic.parameters(), lr=0.01)\n","gamma = 0.95\n","lam = 0.95\n","num_episodes = 150\n","\n","# 학습 진행\n","train_actor_critic_with_gae(env, actor, critic, actor_optimizer, critic_optimizer, gamma, lam, num_episodes)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3x0b1xDGZQ4r","executionInfo":{"status":"ok","timestamp":1715311794567,"user_tz":-540,"elapsed":30144,"user":{"displayName":"임지운","userId":"15598219104552818011"}},"outputId":"fb6d0365-8ecc-4c60-80da-338bc4e084cd"},"execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["Episode 0: Total Reward = 20.0\n","Episode 1: Total Reward = 14.0\n","Episode 2: Total Reward = 34.0\n","Episode 3: Total Reward = 15.0\n","Episode 4: Total Reward = 16.0\n","Episode 5: Total Reward = 12.0\n","Episode 6: Total Reward = 14.0\n","Episode 7: Total Reward = 11.0\n","Episode 8: Total Reward = 13.0\n","Episode 9: Total Reward = 17.0\n","Episode 10: Total Reward = 16.0\n","Episode 11: Total Reward = 11.0\n","Episode 12: Total Reward = 19.0\n","Episode 13: Total Reward = 16.0\n","Episode 14: Total Reward = 15.0\n","Episode 15: Total Reward = 23.0\n","Episode 16: Total Reward = 18.0\n","Episode 17: Total Reward = 21.0\n","Episode 18: Total Reward = 25.0\n","Episode 19: Total Reward = 20.0\n","Episode 20: Total Reward = 13.0\n","Episode 21: Total Reward = 59.0\n","Episode 22: Total Reward = 44.0\n","Episode 23: Total Reward = 37.0\n","Episode 24: Total Reward = 22.0\n","Episode 25: Total Reward = 70.0\n","Episode 26: Total Reward = 28.0\n","Episode 27: Total Reward = 25.0\n","Episode 28: Total Reward = 52.0\n","Episode 29: Total Reward = 54.0\n","Episode 30: Total Reward = 25.0\n","Episode 31: Total Reward = 24.0\n","Episode 32: Total Reward = 56.0\n","Episode 33: Total Reward = 34.0\n","Episode 34: Total Reward = 62.0\n","Episode 35: Total Reward = 36.0\n","Episode 36: Total Reward = 39.0\n","Episode 37: Total Reward = 35.0\n","Episode 38: Total Reward = 66.0\n","Episode 39: Total Reward = 73.0\n","Episode 40: Total Reward = 56.0\n","Episode 41: Total Reward = 133.0\n","Episode 42: Total Reward = 85.0\n","Episode 43: Total Reward = 240.0\n","Episode 44: Total Reward = 120.0\n","Episode 45: Total Reward = 173.0\n","Episode 46: Total Reward = 90.0\n","Episode 47: Total Reward = 143.0\n","Episode 48: Total Reward = 183.0\n","Episode 49: Total Reward = 137.0\n","Episode 50: Total Reward = 179.0\n","Episode 51: Total Reward = 123.0\n","Episode 52: Total Reward = 280.0\n","Episode 53: Total Reward = 135.0\n","Episode 54: Total Reward = 287.0\n","Episode 55: Total Reward = 146.0\n","Episode 56: Total Reward = 85.0\n","Episode 57: Total Reward = 368.0\n","Episode 58: Total Reward = 102.0\n","Episode 59: Total Reward = 290.0\n","Episode 60: Total Reward = 347.0\n","Episode 61: Total Reward = 360.0\n","Episode 62: Total Reward = 221.0\n","Episode 63: Total Reward = 106.0\n","Episode 64: Total Reward = 249.0\n","Episode 65: Total Reward = 262.0\n","Episode 66: Total Reward = 249.0\n","Episode 67: Total Reward = 155.0\n","Episode 68: Total Reward = 140.0\n","Episode 69: Total Reward = 176.0\n","Episode 70: Total Reward = 174.0\n","Episode 71: Total Reward = 161.0\n","Episode 72: Total Reward = 122.0\n","Episode 73: Total Reward = 112.0\n","Episode 74: Total Reward = 140.0\n","Episode 75: Total Reward = 128.0\n","Episode 76: Total Reward = 131.0\n","Episode 77: Total Reward = 123.0\n","Episode 78: Total Reward = 57.0\n","Episode 79: Total Reward = 128.0\n","Episode 80: Total Reward = 155.0\n","Episode 81: Total Reward = 143.0\n","Episode 82: Total Reward = 156.0\n","Episode 83: Total Reward = 161.0\n","Episode 84: Total Reward = 203.0\n","Episode 85: Total Reward = 184.0\n","Episode 86: Total Reward = 200.0\n","Episode 87: Total Reward = 201.0\n","Episode 88: Total Reward = 206.0\n","Episode 89: Total Reward = 227.0\n","Episode 90: Total Reward = 272.0\n","Episode 91: Total Reward = 240.0\n","Episode 92: Total Reward = 424.0\n","Episode 93: Total Reward = 372.0\n","Episode 94: Total Reward = 231.0\n","Episode 95: Total Reward = 463.0\n","Episode 96: Total Reward = 500.0\n","Episode 97: Total Reward = 500.0\n","Episode 98: Total Reward = 500.0\n","Episode 99: Total Reward = 500.0\n","Episode 100: Total Reward = 314.0\n","Episode 101: Total Reward = 125.0\n","Episode 102: Total Reward = 500.0\n","Episode 103: Total Reward = 500.0\n","Episode 104: Total Reward = 500.0\n","Episode 105: Total Reward = 500.0\n","Episode 106: Total Reward = 454.0\n","Episode 107: Total Reward = 500.0\n","Episode 108: Total Reward = 500.0\n","Episode 109: Total Reward = 500.0\n","Episode 110: Total Reward = 350.0\n","Episode 111: Total Reward = 500.0\n","Episode 112: Total Reward = 351.0\n","Episode 113: Total Reward = 500.0\n","Episode 114: Total Reward = 500.0\n","Episode 115: Total Reward = 186.0\n","Episode 116: Total Reward = 500.0\n","Episode 117: Total Reward = 500.0\n","Episode 118: Total Reward = 475.0\n","Episode 119: Total Reward = 500.0\n","Episode 120: Total Reward = 500.0\n","Episode 121: Total Reward = 500.0\n","Episode 122: Total Reward = 500.0\n","Episode 123: Total Reward = 500.0\n","Episode 124: Total Reward = 500.0\n","Episode 125: Total Reward = 500.0\n","Episode 126: Total Reward = 500.0\n","Episode 127: Total Reward = 500.0\n","Episode 128: Total Reward = 500.0\n","Episode 129: Total Reward = 500.0\n","Episode 130: Total Reward = 418.0\n","Episode 131: Total Reward = 493.0\n","Episode 132: Total Reward = 493.0\n","Episode 133: Total Reward = 500.0\n","Episode 134: Total Reward = 224.0\n","Episode 135: Total Reward = 243.0\n","Episode 136: Total Reward = 500.0\n","Episode 137: Total Reward = 500.0\n","Episode 138: Total Reward = 500.0\n","Episode 139: Total Reward = 500.0\n","Episode 140: Total Reward = 500.0\n","Episode 141: Total Reward = 500.0\n","Episode 142: Total Reward = 500.0\n","Episode 143: Total Reward = 500.0\n","Episode 144: Total Reward = 500.0\n","Episode 145: Total Reward = 500.0\n","Episode 146: Total Reward = 367.0\n","Episode 147: Total Reward = 500.0\n","Episode 148: Total Reward = 500.0\n","Episode 149: Total Reward = 500.0\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"HvygCCeybl2k"},"execution_count":null,"outputs":[]}]}